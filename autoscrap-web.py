import asyncio
import json
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright
import pandas as pd
from datetime import datetime
import os
import logging
import sys
import importlib.util
from pathlib import Path
import re

compare_file_path = os.path.join(os.path.dirname(__file__), 'autoscrap-compare.py')
if os.path.exists(compare_file_path):
    spec = importlib.util.spec_from_file_location("data_compare", compare_file_path)
    data_compare = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(data_compare)
else:
    print("Warning: autoscrap-compare.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    data_compare = None

# ë¡œê¹… ì„¤ì • ê°œì„ 
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
)

def ensure_directories():
    os.makedirs("src", exist_ok=True)
    os.makedirs("data", exist_ok=True)
    os.makedirs("data/etc", exist_ok=True)

def load_urls():
    with open("src/urls-web.json") as f:
        data = json.load(f)
    return data["url"]

async def scrape_all_sections():
    all_results = []
    urls = load_urls()    
    brand_map = {
        0: "01_BMW",
        1: "04_Mini",
        2: "02_MB",
        3: "03_Audi",
        4: "12_Volkswagen",
    }

    for i, url in enumerate(urls):
        results = []
        brand = brand_map.get(i, f"Unknown_{i}")        
        print(f"ë¸Œëœë“œ {brand} ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url}")
        
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True, args=['--disable-gpu', '--disable-dev-shm-usage', '--no-sandbox'])
            page = await browser.new_page()
            # í˜ì´ì§€ ìƒì„± í›„ ì´ë¯¸ì§€, ìŠ¤íƒ€ì¼ì‹œíŠ¸, í°íŠ¸ ë“± ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨
            await page.route('**/*.{png,jpg,jpeg,svg,css,woff,woff2}', lambda route: route.abort())
            try:
                await page.goto(url, timeout=60000)
                await page.wait_for_load_state("load")                
                content = await page.content()
                soup = BeautifulSoup(content, "html.parser")

                sections = soup.find_all("section", class_="_1vrlmaf2 _1vrlmaf0")
                for section in sections:
                    section_id = section.get("id", "no-id")
                    
                    # ì‹œë¦¬ì¦ˆ ì´ë¦„ ì¶”ì¶œ
                    series_name = section_id  # ê¸°ë³¸ê°’ìœ¼ë¡œ section_id ì‚¬ìš©
                    section_header = section.select_one("h3.j00ses5")
                    if section_header:
                        header_text_nodes = [text for text in section_header.stripped_strings]
                        
                        if len(header_text_nodes) >= 2:
                            series_name = header_text_nodes[1].strip()
                        elif header_text_nodes:
                            series_name = header_text_nodes[0].strip()
                    
                    # ì—°ì‹ ì •ë³´ ì¶”ì¶œ
                    model_year = "25"  # ê¸°ë³¸ê°’
                    if section_header:
                        year_span = section_header.select_one("span")
                        if year_span:
                            year_text = year_span.get_text(strip=True)
                            year_match = re.search(r'(\d+)ë…„ì‹', year_text)
                            if year_match:
                                model_year = year_match.group(1)
                            else:
                                year_match = re.search(r'20(\d{2})ë…„ì‹', year_text)
                                if year_match:
                                    model_year = year_match.group(1)

                    rows = section.select("a._15c6uvi5, div._15c6uvi5")
                    for row in rows:
                        try:
                            # ëª¨ë¸ëª… ì¶”ì¶œ
                            model_name_elem = row.select_one("span._15c6uvi9")
                            if not model_name_elem:
                                continue
                        
                            model_name = model_name_elem.get_text(strip=True)
                            
                            msrp_elem = row.select_one("div._15c6uvi7 span._15c6uvif")
                            msrp = msrp_elem.get_text(strip=True) if msrp_elem else ""
                            
                            discount_elem = row.select_one("span._15c6uvim._15c6uvif")
                            discount = discount_elem.get_text(strip=True) if discount_elem else "0"
                            
                            logging.info(f"ì¶”ì¶œ: {model_name}, ì¶œê³ ê°€: {msrp}ë§Œì›, í• ì¸: {discount}ë§Œì›")
                            
                            results.append({
                                "Brand": brand,
                                "Series": series_name,
                                "MY": model_year,
                                "Model": model_name,
                                "MSRP": msrp,
                                "Off": discount
                            })
                        except Exception as e:
                            logging.error(f"í–‰ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                            results.append({
                                "Brand": brand,
                                "Series": section_id,
                                "MY": model_year,
                                "Model": "Error",
                                "MSRP": "",
                                "Off": str(e)
                            })
                
                all_results.extend(results)
                print(f"{brand} ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {len(results)}ê°œ í•­ëª©")
            except Exception as e:
                print(f"ì—ëŸ¬ ë°œìƒ: {e}")
            finally:
                await browser.close()
    df = pd.DataFrame(all_results)
    print(df)
    today = datetime.now().strftime("%Y%m%d")
    file_path = f"data/etc/car_data_web_{today}.xlsx"

    # ì´ë¯¸ íŒŒì¼ì´ ì¡´ì¬í•˜ë”ë¼ë„ ìƒˆ ë°ì´í„°ë¡œ ë®ì–´ì“°ê¸° (ê¸°ì¡´ ë°ì´í„°ëŠ” ë²„ë¦¼)
    with pd.ExcelWriter(file_path, engine="openpyxl", mode="w") as writer:
        df.to_excel(writer, index=False)

    logging.info(f"ğŸ’¾ {file_path} ì €ì¥ ì™„ë£Œ. ì´ {len(df)}í–‰ (ê¸°ì¡´ ë°ì´í„° ë®ì–´ì”€).")

if __name__ == "__main__":
    ensure_directories()
    asyncio.run(scrape_all_sections())

    today = datetime.now().strftime("%Y%m%d")
    try:
        if data_compare:
            logging.info("ë°ì´í„° ë¹„êµ ì‹œì‘...")
            data_compare.main(today)
            logging.info("ë°ì´í„° ë¹„êµ ì™„ë£Œ")
        else:
            logging.warning("ë°ì´í„° ë¹„êµ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¹„êµë¥¼ ìˆ˜í–‰í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.error(f"ë°ì´í„° ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")