import asyncio
import os
import json
import pandas as pd
import re
import logging
import sys
from datetime import datetime
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)

def ensure_directories():
    os.makedirs("src", exist_ok=True)
    os.makedirs("data", exist_ok=True)

def load_urls():
    with open("src/urls.json") as f:
        data = json.load(f)
    return data["url"]

def load_existing_data():
    try:
        df = pd.read_excel("car_data.xlsx")
        return df
    except FileNotFoundError:
        df = pd.DataFrame(
            columns=[
                "Year", "Month", "Date", "Brand", "MY",
                "Series", "Fuel Type", "Model (adjusted)",
                "MSRP", "Cash_off", "Finance_off"
            ]
        )
        return df

def save_to_excel(new_data, append=False):
    today = datetime.now().strftime("%Y%m%d")
    file_path = f"data/car_data_{today}.xlsx"
    columns = [
        "Year", "Month", "Date", "Brand", "MY",
        "Series", "Fuel Type", "Model (adjusted)",
        "MSRP", "Cash_off", "Finance_off"
    ]
    
    new_df = pd.DataFrame(new_data, columns=columns)
    
    # ì´ë¯¸ íŒŒì¼ì´ ì¡´ì¬í•˜ê³  append ëª¨ë“œë¼ë©´ ê¸°ì¡´ ë°ì´í„°ì— ì¶”ê°€
    if append and os.path.exists(file_path):
        try:
            existing_df = pd.read_excel(file_path)
            updated_df = pd.concat([existing_df, new_df], ignore_index=True)
            
            with pd.ExcelWriter(file_path, engine="openpyxl", mode="w") as writer:
                updated_df.to_excel(writer, index=False)
                
            logging.info(f"ğŸ’¾ {file_path}ì— {len(new_df)}í–‰ ì¶”ê°€ë¨. ì´ {len(updated_df)}í–‰.")
            return
        except Exception as e:
            logging.error(f"âŒ ê¸°ì¡´ íŒŒì¼ ì½ê¸°/ì¶”ê°€ ì‹¤íŒ¨, ë®ì–´ì“°ê¸°ë¡œ ì§„í–‰í•©ë‹ˆë‹¤: {e}")
    
    # ì²« ì‹¤í–‰ì´ê±°ë‚˜ append ëª¨ë“œê°€ ì•„ë‹ˆë©´ ë®ì–´ì“°ê¸°
    with pd.ExcelWriter(file_path, engine="openpyxl", mode="w") as writer:
        new_df.to_excel(writer, index=False)
    
    logging.info(f"ğŸ’¾ {file_path} ì €ì¥ ì™„ë£Œ. ì´ {len(new_df)}í–‰.")

def fuel_type(x):
    if "íœ˜ë°œìœ " in x:
        return "P"
    elif "ê²½ìœ " in x:
        return "D"
    elif "ì „ê¸°" in x:
        return "BEV"
    elif "í”ŒëŸ¬ê·¸ì¸ í•˜ì´ë¸Œë¦¬ë“œ" in x:
        return "PHEV"
    else:
        return "None"

async def get_car_series(page, brand, is_first_brand=False):
    await page.wait_for_load_state("load")
    content = await page.content()
    soup = BeautifulSoup(content, "html.parser")

    elements = soup.find_all(
        class_="css-175oi2r r-1i6wzkk r-lrvibr r-1loqt21 r-1otgn73 r-1awozwy r-18u37iz r-1wtj0ep r-117bsoe r-11wrixw r-61z16t r-1x0uki6 r-1mdbw0j r-1hfyk0a r-1qfoi16 r-wk8lta r-13qz1uu"
    )
    if not elements:
        logging.warning(f"{brand} ì‹œë¦¬ì¦ˆ ìš”ì†Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return []

    today = datetime.now()
    all_series_data = []  # ëª¨ë“  ì‹œë¦¬ì¦ˆ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

    for element in elements:
        try:
            car_series = element.find(
                "div",
                class_="css-146c3p1 r-1jstmqa r-litx2b r-1b43r93 r-icto9i r-14yzgew r-p76n7o r-13wfysu r-1a2p6p6",
            ).get_text(strip=True)
        except Exception:
            continue

        logging.info(f"ì‹œë¦¬ì¦ˆ íƒìƒ‰ ì¤‘: {car_series}")

        existing_data = load_existing_data()
        

        if not existing_data[
            (existing_data["Year"] == today.year)
            & (existing_data["Month"] == today.month)
            & (existing_data["Date"] == today.day)
            & (existing_data["Series"] == car_series)
        ].empty:
            logging.info(f"â© {car_series} ì´ë¯¸ ìˆ˜ì§‘ë¨. ìŠ¤í‚µ.")
            continue

        # ê° ì‹œë¦¬ì¦ˆ ë°ì´í„° ìˆ˜ì§‘
        series_data = await get_car_info(page, car_series, brand)
        if series_data:
            logging.info(f"âœ… {car_series} ìˆ˜ì§‘ ì™„ë£Œ, {len(series_data)}ê°œ í•­ëª© ìˆ˜ì§‘")
            all_series_data.extend(series_data)  # ì „ì²´ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            
            # ì‹œë¦¬ì¦ˆ ìˆ˜ì§‘ í›„ ë°”ë¡œ ì €ì¥ (ì²« ë²ˆì§¸ ë¸Œëœë“œì˜ ì²« ë²ˆì§¸ ì‹œë¦¬ì¦ˆëŠ” ë®ì–´ì“°ê¸°, ë‚˜ë¨¸ì§€ëŠ” ì¶”ê°€)
            append_mode = not (is_first_brand and len(all_series_data) == len(series_data))
            save_to_excel(series_data, append=append_mode)
            logging.info(f"ğŸ’¾ {car_series} ë°ì´í„° {len(series_data)}ê°œ í•­ëª© ì €ì¥ ì™„ë£Œ")
        else:
            logging.warning(f"âš ï¸ {car_series} ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")

        await page.wait_for_load_state("load")

    return all_series_data

async def get_car_info(page, car_series, brand):
    try:
        series_locator = page.locator(f"text={car_series}").first
        await series_locator.click()
        await page.wait_for_load_state("load")

        # ë¸Œëœë“œ í˜ì´ì§€ iframe ì ‘ê·¼
        if "5" in car_series:
            elements_locator = page.locator("div.sc-80108d2f-0.hlytKE")
        else: 
            brand_iframe_locator = page.frame_locator("iframe[src*='https://cd.getcha.kr/brand/']")
            elements_locator = brand_iframe_locator.locator("div.sc-80108d2f-0.hlytKE")

        await elements_locator.first.wait_for(timeout=3000)

        count = await elements_locator.count()
        if count == 0:
            logging.warning(f"âš ï¸ í• ì¸ë˜ëŠ” {car_series} ëª¨ë¸ ì—†ìŒ. ë‹¤ìŒ ì‹œë¦¬ì¦ˆë¡œ ì´ë™.")
            return []

        series_car_data = []

        for i in range(count):
            try:
                el = elements_locator.nth(i)
                await el.scroll_into_view_if_needed()
                await page.wait_for_load_state("load")

                # iframe ì „ì²´ HTML íŒŒì‹±
                brand_frame = next((f for f in page.frames if "https://cd.getcha.kr/brand/" in f.url), None)
                if not brand_frame:
                    raise Exception("âŒ ë¸Œëœë“œ iframe ë¡œë“œ ì‹¤íŒ¨")

                content = await brand_frame.content()
                soup = BeautifulSoup(content, "html.parser")
                
                # DOM íŠ¸ë¦¬ ìˆœíšŒ ëŒ€ì‹  ì§ì ‘ ì„ íƒ
                car_model_tags = []
                all_h5s = soup.select("h5.sc-850306bd-6.DcjFc")
                for h5 in all_h5s:
                    parent_div = h5.find_parent("div", class_="sc-80108d2f-0")
                    if parent_div and "hlytKE" in parent_div["class"] and "kwqkHl" not in parent_div["class"]:
                        car_model_tags.append(h5)

                if i >= len(car_model_tags):
                    raise Exception("âŒ car_model ì—˜ë¦¬ë¨¼íŠ¸ ë¶€ì¡±")

                car_model_tag = car_model_tags[i]
                car_model = car_model_tag.get_text(strip=True)

                # ê°€ì¥ ê°€ê¹Œìš´ ë¶€ëª¨ ë¸”ë¡ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰
                fuel_parent_block = car_model_tag.find_parent("div", class_="sc-84b91bcb-0 fscxQt")
                year_parent_block = car_model_tag.find_parent("div", class_="sc-16e7f35c-0 iTBJvM")

                car_year_tag = year_parent_block.select_one("div.sc-16e7f35c-1.bEkQLM h4.sc-850306bd-5.iXDDjz")
                car_year = car_year_tag.get_text(strip=True)[2:4] if car_year_tag else "00"

                car_fuel_tag = fuel_parent_block.select_one("div.sc-84b91bcb-1.dpHZpA h6.sc-850306bd-8.bcvqMy")
                car_fuel = fuel_type(car_fuel_tag.get_text(strip=True)) if car_fuel_tag else "Unknown"


                logging.info(f"{car_series} - {car_model} ({car_year}) {car_fuel}")
                await el.click()
                await page.wait_for_load_state("domcontentloaded")

                model_data = await get_car_price(page, car_model, car_series, car_year, car_fuel, brand)
                if model_data:
                    series_car_data.append(model_data)

            except Exception as e:
                logging.warning(f"âš ï¸ {car_series} ëª¨ë¸ {i+1}/{count} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

            finally:
                try:
                    if not page.is_closed():
                        await page.go_back()
                        await page.wait_for_load_state("load")
                except Exception as e:
                    logging.warning(f"âš ï¸ ë’¤ë¡œê°€ê¸° ì‹¤íŒ¨: {e}")
        await page.go_back()
        return series_car_data

    except PlaywrightTimeoutError:
        logging.error(f"âŒ {car_series} ëª¨ë¸ í´ë¦­ ì‹¤íŒ¨ ë˜ëŠ” ìš”ì†Œ ë¡œë“œ ì‹¤íŒ¨")
        try:
            if not page.is_closed():
                await page.go_back()
                await page.wait_for_load_state("load")
        except:
            pass
        return []


async def get_car_price(frame, car_model, car_series, car_year, car_fuel, brand):
    try:
        await frame.wait_for_selector("iframe[src*='car-detail']", timeout=10000)
        await frame.wait_for_load_state("load")

        detail_frame = None
        for f in frame.frames:
            try:
                el = await f.frame_element()
                src = await el.get_attribute("src")
                if src and "car-detail" in src:
                    detail_frame = f
                    break
            except:
                continue

        if not detail_frame:
            raise Exception("âŒ car-detail iframeì„ src ê¸°ë°˜ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        await detail_frame.wait_for_selector("div.sc-68368f62-0.gfdAnO", timeout=6000)
        content = await detail_frame.content()
        soup = BeautifulSoup(content, "html.parser")

        msrp_element = soup.select_one("#cardetail_container > div.sc-68368f62-0.gfdAnO > div > div:nth-child(1) > div")
        cash_off_element = soup.select_one("#cardetail_container > div.sc-68368f62-0.gfdAnO > div > div:nth-child(2) > em")
        finance_off_element = soup.select_one("#cardetail_container > div.sc-68368f62-0.gfdAnO > div > div:nth-child(3) > em")

        msrp = msrp_element.get_text(strip=True).replace("ë§Œì›", "") if msrp_element else "N/A"
        cash_off = re.search(r"([0-9,]+)ë§Œì›", cash_off_element.get_text(strip=True)).group(1).replace(",", "") if cash_off_element else "0"
        finance_off = re.search(r"([0-9,]+)ë§Œì›", finance_off_element.get_text(strip=True)).group(1).replace(",", "") if finance_off_element else "0"

        cash_off = f"{int(cash_off):,}"
        finance_off = f"{int(finance_off):,}"

        logging.info(f"ê°€ê²©: {msrp}ë§Œì›, í˜„ê¸ˆí• ì¸: {cash_off}ë§Œì›, ê¸ˆìœµí• ì¸: {finance_off}ë§Œì›")

        if cash_off != "0" or finance_off != "0":
            return [
                datetime.now().year,
                datetime.now().month,
                datetime.now().day,
                brand,
                car_year,
                car_series,
                car_fuel,
                car_model,
                msrp,
                cash_off,
                finance_off,
            ]
    except Exception as e:
        logging.error(f"âŒ {car_model} ê°€ê²© íŒŒì‹± ì‹¤íŒ¨: {e}")
    return None

def find_parent_with_class(element, class_name):
    parent = element.parent
    while parent:
        if parent.name == "div" and class_name in parent.get("class", []):
            return parent
        parent = parent.parent
    return None

async def main():
    ensure_directories()
    url = load_urls()
    brand_map = {
        1: "01_BMW",
        2: "04_Mini",
        3: "02_MB",
        4: "03_Audi",
        5: "12_Volkswagen",
    }

    all_data = []
    first_brand = True

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True, args=['--disable-gpu', '--disable-dev-shm-usage', '--no-sandbox'])
        context = await browser.new_context()

        for i in range(1, 6):
            brand = brand_map[i]
            max_retries = 5  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            retries = 0
            brand_data = []
            
            while retries < max_retries and len(brand_data) == 0:
                if retries > 0:
                    logging.warning(f"âš ï¸ {brand} ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨, {retries}ë²ˆì§¸ ì¬ì‹œë„ ì¤‘...")
                    await asyncio.sleep(3)  # ì¬ì‹œë„ ì „ ì ì‹œ ëŒ€ê¸°
                
                page = await context.new_page()
                # í˜ì´ì§€ ìƒì„± í›„ ì´ë¯¸ì§€, ìŠ¤íƒ€ì¼ì‹œíŠ¸, í°íŠ¸ ë“± ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨
                await page.route('**/*.{png,jpg,jpeg,svg,css,woff,woff2}', lambda route: route.abort())
                try:
                    await page.goto(url[i], timeout=600000)
                    await page.wait_for_load_state("load")
                    
                    # BMW ë¸Œëœë“œëŠ” ë” ì˜¤ë˜ ê¸°ë‹¤ë¦¼
                    if i == 1:  # BMW
                        await asyncio.sleep(5)
                    else:
                        await asyncio.sleep(3)
                    
                    logging.info(f"\n====== ë¸Œëœë“œ ì‹œì‘: {brand} ({retries+1}ë²ˆì§¸ ì‹œë„) ======")
                    
                    # ë¸Œëœë“œë³„ ë°ì´í„° ìˆ˜ì§‘ (ì²« ë²ˆì§¸ ë¸Œëœë“œëŠ” is_first_brand=Trueë¡œ ì „ë‹¬)
                    brand_data = await get_car_series(page, brand, is_first_brand=first_brand)
                    
                    if len(brand_data) > 0:
                        logging.info(f"âœ… ë¸Œëœë“œ {brand} ë°ì´í„° {len(brand_data)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
                        all_data.extend(brand_data)
                        break  # ë°ì´í„°ê°€ ìˆ˜ì§‘ë˜ì—ˆìœ¼ë©´ ì¬ì‹œë„ ë£¨í”„ ì¢…ë£Œ
                    else:
                        logging.warning(f"âš ï¸ {brand} ë°ì´í„° 0ê°œ ìˆ˜ì§‘ë¨, ì¬ì‹œë„ í•„ìš”")
                        retries += 1
                        
                except Exception as e:
                    logging.error(f"âŒ {brand} ì˜¤ë¥˜ ë°œìƒ: {e}")
                    retries += 1
                finally:
                    await page.close()
            
            if len(brand_data) == 0:
                logging.error(f"âŒ {brand} ë°ì´í„° ìˆ˜ì§‘ ìµœì¢… ì‹¤íŒ¨. ë‹¤ìŒ ë¸Œëœë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            
            first_brand = False  # ì²« ë²ˆì§¸ ë¸Œëœë“œ ì²˜ë¦¬ í›„ í”Œë˜ê·¸ ë³€ê²½

        await browser.close()

    logging.info(f"ğŸ’¾ ì „ì²´ ë°ì´í„° {len(all_data)}ê°œ í•­ëª© ìˆ˜ì§‘ ì™„ë£Œ")

    # autoscrap-web.py ì‹¤í–‰
    logging.info("\n=== autoscrap.py ì™„ë£Œ. autoscrap-web.py ì‹¤í–‰ ì¤‘... ===")
    try:
        # í•˜ìœ„ í”„ë¡œì„¸ìŠ¤ë¡œ autoscrap-web.py ì‹¤í–‰
        import subprocess
        web_script_path = os.path.join(os.path.dirname(__file__), 'autoscrap-web.py')
        if os.path.exists(web_script_path):
            subprocess.run([sys.executable, web_script_path], check=True)
            logging.info("âœ… autoscrap-web.py ì‹¤í–‰ ì™„ë£Œ")
        else:
            logging.error("âŒ autoscrap-web.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        logging.error(f"âŒ autoscrap-web.py ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    asyncio.run(main())